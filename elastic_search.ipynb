{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the reason i am trying to do it with elastic search is that docker doesnt work with my pc, it just crashes. \n",
    "\n",
    "I also ran into problems when importing txtai, even after installing everything from scratch, so I decided to try in colab, and there it works. I am now trying to upload from colab to the local elastic search. \n",
    "\n",
    "Got same problem when trying to access the pubmedBERT from transformers, so dont know what i did wrong, pls try and see if it works for you :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install txtai\n",
    "# !pip install --upgrade numpy transformers # to not get an error when importing txtai, but didnt work\n",
    "!pip install elasticsearch transformers torch pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_token' from 'huggingface_hub.utils' (c:\\Users\\isakb\\Anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melasticsearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Elasticsearch\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melasticsearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bulk\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32mc:\\Users\\isakb\\Anaconda3\\lib\\site-packages\\sentence_transformers\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32mc:\\Users\\isakb\\Anaconda3\\lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n",
      "File \u001b[1;32mc:\\Users\\isakb\\Anaconda3\\lib\\site-packages\\sentence_transformers\\datasets\\ParallelSentencesDataset.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n",
      "File \u001b[1;32mc:\\Users\\isakb\\Anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi, HfFolder, Repository, hf_hub_url, cached_download\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor, device\n",
      "File \u001b[1;32mc:\\Users\\isakb\\Anaconda3\\lib\\site-packages\\huggingface_hub\\__init__.py:332\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attach\u001b[39m(package_name, submodules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, submod_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    323\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Attach lazily loaded submodules, functions, or other attributes.\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    Typically, modules import submodules and attributes as follows:\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    import mysubmodule\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m    import anothersubmodule\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m    from .foo import someattr\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    The idea is to replace a package's `__getattr__`, `__dir__`, and\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    `__all__`, such that all imports work exactly the way they would\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    with normal imports, except that the import occurs upon first use.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    The typical way to call this function, replacing the above imports, is:\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    ```python\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    __getattr__, __dir__, __all__ = lazy.attach(\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m        __name__,\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m        ['mysubmodule', 'anothersubmodule'],\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m        {'foo': ['someattr']}\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    )\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03m    This functionality requires Python 3.7 or higher.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m        package_name (`str`):\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m            Typically use `__name__`.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        submodules (`set`):\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m            List of submodules to attach.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        submod_attrs (`dict`):\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m            Dictionary of submodule -> list of attributes / functions.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m            These attributes are imported as they are used.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        __getattr__, __dir__, __all__\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m submod_attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    363\u001b[0m         submod_attrs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\isakb\\Anaconda3\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\isakb\\Anaconda3\\lib\\site-packages\\huggingface_hub\\repository.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi, repo_type_and_id_from_hf_id\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LFS_MULTIPART_UPLOAD_COMMAND\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     SoftTemporaryDirectory,\n\u001b[0;32m     19\u001b[0m     get_token,\n\u001b[0;32m     20\u001b[0m     logging,\n\u001b[0;32m     21\u001b[0m     run_subprocess,\n\u001b[0;32m     22\u001b[0m     tqdm,\n\u001b[0;32m     23\u001b[0m     validate_hf_hub_args,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_method\n\u001b[0;32m     28\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_token' from 'huggingface_hub.utils' (c:\\Users\\isakb\\Anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import txtai # dont know why is doesnt work in this env when i have installed everything in the notebook\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "from sentence_transformers import SentenceTransformer # dont know why any of them work, think my env is f-ed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PMID                                              Title  \\\n",
      "0  38336420  The impact of austerity on children: Uncoverin...   \n",
      "1  38333244  Artificial intelligence-powered intraoperative...   \n",
      "2  38333196  Helix-based screening with structure predictio...   \n",
      "3  38333180  Role of robotics and artificial intelligence i...   \n",
      "4  38333148  Relationship between spiritual intelligence an...   \n",
      "\n",
      "                                            Abstract Publication Year  \n",
      "0  Which children are most vulnerable when their ...             2024  \n",
      "1                              No abstract available             2024  \n",
      "2  The rapid development of drugs against emergin...             2024  \n",
      "3  Artificial intelligence or AI may be identifie...             2023  \n",
      "4  The COVID-19 pandemic has caused physical and ...             2023  \n",
      "9997\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"pubmed_articles_first_9999.csv\")\n",
    "\n",
    "print(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccessing\n",
    "\n",
    "Expand later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PMID   Title  Abstract  Publication Year\n",
       "False  False  False     False               9835\n",
       "              True      False                140\n",
       "       True   False     False                 20\n",
       "              True      False                  2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This is the performance of the different models on huggingface, so use the pubmed bert__\n",
    "\n",
    "Model\tPubMed QA\tPubMed Subset\tPubMed Summary\tAverage\n",
    "\n",
    "all-MiniLM-L6-v2\t90.40\t95.86\t94.07\t93.44\n",
    "\n",
    "bge-base-en-v1.5\t91.02\t95.60\t94.49\t93.70\n",
    "\n",
    "gte-base\t92.97\t96.83\t96.24\t95.35\n",
    "\n",
    "pubmedbert-base-embeddings\t93.27\t97.07\t96.58\t95.64\n",
    "\n",
    "S-PubMedBert-MS-MARCO\t90.86\t93.33\t93.54\t92.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare documents from DataFrame\n",
    "def documents(df):\n",
    "    # Combine title and abstract for the text content, use PMID as the document ID\n",
    "    return [{\"id\": str(row[\"PMID\"]), \"text\": f\"{row['Title']} {row['Abstract']}\"} for index, row in df.iterrows()]\n",
    "\n",
    "# Initialize txtai embeddings instance with PubMedBERT\n",
    "embeddings = txtai.Embeddings({\"path\": \"neuml/pubmedbert-base-embeddings\", \"content\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Index the documents\n",
    "embeddings.index(documents(df))\n",
    "\n",
    "# Example search query\n",
    "results = embeddings.search(\"Artificial intelligence in healthcare\", limit=5)\n",
    "\n",
    "# Display search results\n",
    "for result in results:\n",
    "    # result = (id, score)\n",
    "    # Fetch the document using result[0] as the index into df\n",
    "    document = df.loc[df['PMID'] == int(result[0])]\n",
    "    print(f\"PMID: {document.iloc[0]['PMID']}, Title: {document.iloc[0]['Title']}, Score: {result[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()[0]\n",
    "\n",
    "# Generate embeddings for a column (e.g., Abstract)\n",
    "df['embedding'] = df['Abstract'].apply(lambda x: generate_embeddings(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "index_name = \"pubmed_articles_embeddings\"\n",
    "\n",
    "# Create index with mapping for dense_vector\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body={\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"text\"},\n",
    "                \"abstract\": {\"type\": \"text\"},\n",
    "                \"publication_year\": {\"type\": \"keyword\"},\n",
    "                \"embedding\": {\"type\": \"dense_vector\", \"dims\": 768}\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Prepare documents for indexing\n",
    "docs = [\n",
    "    {\n",
    "        \"_index\": index_name,\n",
    "        \"_id\": row[\"PMID\"],\n",
    "        \"_source\": {\n",
    "            \"title\": row[\"Title\"],\n",
    "            \"abstract\": row[\"Abstract\"],\n",
    "            \"publication_year\": row[\"Publication Year\"],\n",
    "            \"embedding\": row[\"embedding\"].tolist(),\n",
    "        }\n",
    "    }\n",
    "    for index, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Index documents\n",
    "bulk(es, docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = generate_embeddings(\"How does sickness affect intelligence?\")\n",
    "\n",
    "# Script to calculate cosine similarity\n",
    "script_query = {\n",
    "    \"script_score\": {\n",
    "        \"query\": {\"match_all\": {}},\n",
    "        \"script\": {\n",
    "            \"source\": \"cosineSimilarity(params.query_vector, doc['embedding']) + 1.0\",\n",
    "            \"params\": {\"query_vector\": query_vector.tolist()}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=index_name, body={\"query\": script_query})\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"ID: {hit['_id']}, Score: {hit['_score']}, Title: {hit['_source']['title']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
